{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\swp99\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\swp99\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: \n",
      "['P' 'P' 'P' 'P' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S'\n",
      " 'S' 'S' 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'P' 'P' 'P' 'P' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E'\n",
      " 'N' 'E' 'S' 'E' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'W' 'W' 'W' 'W' 'E' 'E'\n",
      " 'E' 'E' 'N' 'E' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W'\n",
      " 'E' 'E' 'E' 'E' 'W' 'N' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W'\n",
      " 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N'\n",
      " 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N'\n",
      " 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'W']\n",
      "Number of Policy Iterations: 18\n",
      "Optimal Value Function: \n",
      "[ 4.16666667e+01  2.82936746e+00  1.41146667e+01  4.78670933e+00\n",
      " -3.68645011e+00  2.82936746e+00 -3.68645011e+00 -2.43447287e+00\n",
      "  2.82936747e+00 -1.79309109e+00  1.41146667e+01 -9.91363857e-01\n",
      " -2.94757829e+00 -1.79309109e+00 -2.94757829e+00  4.78670933e+00\n",
      "  5.33333333e+01  4.78670933e+00  1.88933333e+01  7.23338667e+00\n",
      "  3.23333333e+01  1.26349397e+00  1.02917333e+01  2.82936747e+00\n",
      " -3.35806264e+00  4.78670933e+00 -3.35806264e+00 -1.79309109e+00\n",
      "  1.26349397e+00 -2.43447287e+00  1.02917333e+01 -1.79309109e+00\n",
      " -2.43447287e+00 -9.91363862e-01 -2.43447287e+00  7.23338667e+00\n",
      "  4.16666667e+01  7.23338666e+00  1.41146667e+01  1.02917333e+01\n",
      "  7.23338667e+00 -2.94757830e+00  1.07951787e-02 -2.43447287e+00\n",
      "  1.07951787e-02  2.48666667e+01  1.07951787e-02  4.78670933e+00\n",
      "  1.07951787e-02 -2.94757830e+00  7.23338667e+00 -2.43447287e+00\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  1.02917333e+01  3.23333333e+01  1.02917333e+01  1.41146667e+01\n",
      "  4.78670933e+00 -3.35806264e+00 -9.91363857e-01 -2.94757829e+00\n",
      "  1.26349397e+00  3.23333333e+01  1.26349397e+00  7.23338667e+00\n",
      " -9.91363857e-01 -3.35806264e+00  4.78670933e+00 -2.94757829e+00\n",
      " -9.91363857e-01  1.26349397e+00 -9.91363857e-01  1.41146667e+01\n",
      "  7.23338667e+00  4.16666667e+01  7.23338667e+00  1.88933333e+01\n",
      "  2.82936747e+00 -3.68645011e+00 -1.79309109e+00 -3.35806264e+00\n",
      "  2.82936747e+00  4.16666667e+01  2.82936747e+00  1.02917333e+01\n",
      " -1.79309109e+00 -3.68645011e+00  2.82936747e+00 -3.35806264e+00\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  4.78670933e+00  5.33333333e+01  4.78670933e+00  1.41146667e+01\n",
      "  3.23333333e+01  1.26349397e+00  1.02917333e+01  2.82936747e+00\n",
      " -3.35806264e+00  4.78670933e+00 -3.35806264e+00 -1.79309109e+00\n",
      "  4.78670933e+00 -9.91363862e-01  1.88933333e+01  1.07951787e-02\n",
      " -2.43447287e+00 -9.91363862e-01 -2.43447287e+00  7.23338667e+00\n",
      "  4.16666667e+01  7.23338666e+00  2.48666667e+01  1.02917333e+01\n",
      "  2.48666667e+01  1.07951748e-02  7.23338667e+00  1.26349397e+00\n",
      " -2.94757829e+00  7.23338666e+00 -2.94757829e+00 -9.91363857e-01\n",
      "  2.82936747e+00 -1.79309109e+00  1.41146667e+01 -9.91363857e-01\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  3.23333333e+01  1.02917333e+01  1.88933333e+01  1.41146667e+01\n",
      "  1.02917333e+01 -2.43447287e+00  1.26349397e+00 -1.79309109e+00\n",
      " -9.91363857e-01  1.88933333e+01 -9.91363857e-01  2.82936747e+00\n",
      "  1.26349397e+00 -2.43447287e+00  1.02917333e+01 -1.79309109e+00\n",
      " -9.91363857e-01  1.26349397e+00 -9.91363857e-01  1.41146667e+01\n",
      "  1.41146667e+01  2.48666667e+01  1.41146667e+01  1.88933333e+01\n",
      "  7.23338667e+00 -2.94757830e+00  1.07951787e-02 -2.43447287e+00\n",
      "  1.07951787e-02  2.48666667e+01  1.07951787e-02  4.78670933e+00\n",
      "  1.07951787e-02 -2.94757830e+00  7.23338667e+00 -2.43447287e+00\n",
      "  1.07951787e-02  2.82936746e+00  1.07951787e-02  1.88933333e+01\n",
      "  1.02917333e+01  3.23333333e+01  1.02917333e+01  2.48666667e+01\n",
      "  4.78670933e+00 -3.35806264e+00 -9.91363857e-01 -2.94757829e+00\n",
      "  1.26349397e+00  3.23333333e+01  1.26349397e+00  7.23338667e+00\n",
      " -9.91363857e-01 -3.35806264e+00  4.78670933e+00 -2.94757829e+00\n",
      " -9.91363857e-01  1.26349397e+00 -9.91363857e-01  1.41146667e+01\n",
      "  7.23338667e+00  4.16666667e+01  7.23338667e+00  1.88933333e+01\n",
      "  2.48666667e+01  1.07951748e-02  7.23338667e+00  1.26349397e+00\n",
      " -2.94757829e+00  7.23338666e+00 -2.94757829e+00 -9.91363857e-01\n",
      "  7.23338667e+00  1.07951748e-02  2.48666667e+01  1.26349397e+00\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  3.23333333e+01  1.02917333e+01  3.23333333e+01  1.41146667e+01\n",
      "  1.88933333e+01 -9.91363862e-01  4.78670933e+00  1.07951787e-02\n",
      " -2.43447287e+00  1.02917333e+01 -2.43447287e+00  1.07951787e-02\n",
      "  4.78670933e+00 -9.91363862e-01  1.88933333e+01  1.07951787e-02\n",
      " -9.91363857e-01  1.26349397e+00 -9.91363857e-01  1.41146667e+01\n",
      "  2.48666667e+01  1.41146667e+01  2.48666667e+01  1.88933333e+01\n",
      "  1.41146667e+01 -1.79309109e+00  2.82936747e+00 -9.91363857e-01\n",
      " -1.79309109e+00  1.41146667e+01 -1.79309109e+00  1.26349397e+00\n",
      "  2.82936747e+00 -1.79309109e+00  1.41146667e+01 -9.91363857e-01\n",
      "  1.07951787e-02  2.82936746e+00  1.07951787e-02  1.88933333e+01\n",
      "  1.88933333e+01  1.88933333e+01  1.88933333e+01  2.48666667e+01\n",
      "  1.02917333e+01 -2.43447287e+00  1.26349397e+00 -1.79309109e+00\n",
      " -9.91363857e-01  1.88933333e+01 -9.91363857e-01  2.82936747e+00\n",
      "  1.26349397e+00 -2.43447287e+00  1.02917333e+01 -1.79309109e+00\n",
      "  1.26349397e+00  4.78670933e+00  1.26349397e+00  2.48666667e+01\n",
      "  1.41146667e+01  2.48666667e+01  1.41146667e+01  3.23333333e+01\n",
      "  7.23338667e+00 -2.94757830e+00  1.07951787e-02 -2.43447287e+00\n",
      "  1.07951787e-02  2.48666667e+01  1.07951787e-02  4.78670933e+00\n",
      "  1.07951787e-02 -2.94757830e+00  7.23338667e+00 -2.43447287e+00\n",
      "  1.07951787e-02  2.82936746e+00  1.07951787e-02  1.88933333e+01\n",
      "  1.02917333e+01  3.23333333e+01  1.02917333e+01  2.48666667e+01\n",
      "  1.88933333e+01 -9.91363862e-01  4.78670933e+00  1.07951787e-02\n",
      " -3.35806264e+00  4.78670933e+00 -3.35806264e+00 -1.79309109e+00\n",
      "  1.02917333e+01  1.26349397e+00  3.23333333e+01  2.82936747e+00\n",
      " -2.43447287e+00 -9.91363862e-01 -2.43447287e+00  7.23338667e+00\n",
      "  2.48666667e+01  7.23338666e+00  4.16666667e+01  1.02917333e+01\n",
      "  1.41146667e+01 -1.79309109e+00  2.82936747e+00 -9.91363857e-01\n",
      " -2.94757829e+00  7.23338666e+00 -2.94757829e+00 -9.91363857e-01\n",
      "  2.82936747e+00 -1.79309109e+00  1.41146667e+01 -9.91363857e-01\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  1.88933333e+01  1.02917333e+01  1.88933333e+01  1.41146667e+01\n",
      "  1.02917333e+01 -2.43447287e+00  1.26349397e+00 -1.79309109e+00\n",
      " -2.43447287e+00  1.02917333e+01 -2.43447287e+00  1.07951787e-02\n",
      "  1.26349397e+00 -2.43447287e+00  1.02917333e+01 -1.79309109e+00\n",
      " -9.91363857e-01  1.26349397e+00 -9.91363857e-01  1.41146667e+01\n",
      "  1.41146667e+01  1.41146667e+01  1.41146667e+01  1.88933333e+01\n",
      "  7.23338667e+00 -2.94757830e+00  1.07951787e-02 -2.43447287e+00\n",
      " -1.79309109e+00  1.41146667e+01 -1.79309109e+00  1.26349397e+00\n",
      "  1.07951787e-02 -2.94757830e+00  7.23338667e+00 -2.43447287e+00\n",
      "  2.82936747e+00  7.23338666e+00  2.82936747e+00  3.23333333e+01\n",
      "  1.02917333e+01  1.88933333e+01  1.02917333e+01  4.16666667e+01\n",
      "  4.78670933e+00 -3.35806264e+00 -9.91363857e-01 -2.94757829e+00\n",
      " -9.91363857e-01  1.88933333e+01 -9.91363857e-01  2.82936747e+00\n",
      " -9.91363857e-01 -3.35806264e+00  4.78670933e+00 -2.94757829e+00\n",
      "  1.26349397e+00  4.78670933e+00  1.26349397e+00  2.48666667e+01\n",
      "  7.23338667e+00  2.48666667e+01  7.23338667e+00  3.23333333e+01\n",
      "  1.41146667e+01 -1.79309109e+00  2.82936747e+00 -9.91363857e-01\n",
      " -3.68645011e+00  2.82936746e+00 -3.68645011e+00 -2.43447287e+00\n",
      "  1.41146667e+01  2.82936746e+00  4.16666667e+01  4.78670933e+00\n",
      " -2.94757829e+00 -1.79309109e+00 -2.94757829e+00  4.78670933e+00\n",
      "  1.88933333e+01  4.78670933e+00  5.33333333e+01  7.23338667e+00\n",
      "  1.02917333e+01 -2.43447287e+00  1.26349397e+00 -1.79309109e+00\n",
      " -3.35806264e+00  4.78670933e+00 -3.35806264e+00 -1.79309109e+00\n",
      "  1.26349397e+00 -2.43447287e+00  1.02917333e+01 -1.79309109e+00\n",
      " -2.43447287e+00 -9.91363862e-01 -2.43447287e+00  7.23338667e+00\n",
      "  1.41146667e+01  7.23338666e+00  1.41146667e+01  1.02917333e+01\n",
      "  7.23338667e+00 -2.94757830e+00  1.07951787e-02 -2.43447287e+00\n",
      " -2.94757829e+00  7.23338666e+00 -2.94757829e+00 -9.91363857e-01\n",
      "  1.07951787e-02 -2.94757830e+00  7.23338667e+00 -2.43447287e+00\n",
      " -1.79309109e+00  1.07951748e-02 -1.79309109e+00  1.02917333e+01\n",
      "  1.02917333e+01  1.02917333e+01  1.02917333e+01  1.41146667e+01\n",
      "  4.78670933e+00 -3.35806264e+00 -9.91363857e-01 -2.94757829e+00\n",
      " -2.43447287e+00  1.02917333e+01 -2.43447287e+00  1.07951787e-02\n",
      " -9.91363857e-01 -3.35806264e+00  4.78670933e+00 -2.94757829e+00\n",
      "  4.78670933e+00  1.02917333e+01  4.78670933e+00  4.16666667e+01\n",
      "  7.23338667e+00  1.41146667e+01  7.23338667e+00  5.33333333e+01\n",
      "  2.82936747e+00 -3.68645011e+00 -1.79309109e+00 -3.35806264e+00\n",
      " -1.79309109e+00  1.41146667e+01 -1.79309109e+00  1.26349397e+00\n",
      " -1.79309109e+00 -3.68645011e+00  2.82936747e+00 -3.35806264e+00\n",
      "  2.82936747e+00  7.23338666e+00  2.82936747e+00  3.23333333e+01\n",
      "  4.78670933e+00  1.88933333e+01  4.78670933e+00  4.16666667e+01]\n"
     ]
    }
   ],
   "source": [
    "# Samuel Price\n",
    "# 02/25/2021\n",
    "# RL&C HW #1\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Resources\n",
    "#  Implemented learning techniques from the University of Alberta\n",
    "#  URL: https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/policy-iteration-Xv32P\n",
    "\n",
    "#  Modified a few functions from allanbreyes\n",
    "#  Repository: https://github.com/allanbreyes/gym-solutions/blob/master/analysis/mdp.py\n",
    "\n",
    "# Load in Gym Taxi\n",
    "taxi = gym.make('Taxi-v3')\n",
    "\n",
    "# Create Mapping for Taxi Problem\n",
    "mapping = {0: \"S\", 1: \"N\", 2: \"E\", 3: \"W\", 4: \"P\", 5: \"D\"}\n",
    "\n",
    "# Get the total number of stages and actions from the environment\n",
    "s_count = taxi.observation_space.n\n",
    "a_count = taxi.action_space.n\n",
    "\n",
    "# Set intial policy using the sample policy from the environment instead of a completely blank slate\n",
    "policy = np.array([taxi.action_space.sample() for i in range(s_count)])\n",
    "\n",
    "# Set value function to all zeros for each stage\n",
    "v_function = np.zeros(s_count)\n",
    "\n",
    "# Get the Rewards and Transitions from the Environment\n",
    "Rewards = np.zeros((s_count, a_count, s_count))\n",
    "Transitions = np.zeros((s_count, a_count, s_count))\n",
    "\n",
    "# Set each reward and transition probability value based on the given environment\n",
    "for state in range(0,s_count):\n",
    "    for action in range(0,a_count):\n",
    "        for transition in taxi.env.P[state][action]:\n",
    "            prob, next_state, reward, not_used = transition\n",
    "            Rewards[state, action, next_state] = reward\n",
    "            Transitions[state, action, next_state] = prob\n",
    "        \n",
    "# Perform Policy Iteration\n",
    "# Set Maximum iterations to 1000\n",
    "# Used a delta of 0.0001 and gamma of .8\n",
    "\n",
    "# Policy Iteration Loop\n",
    "for i in range(1,1000):\n",
    "    previous_policy = policy.copy()\n",
    "    \n",
    "    # Value Function Iteration Loop\n",
    "    for j in range(1, 1000):\n",
    "        \n",
    "        previous_v_function = v_function.copy()\n",
    "\n",
    "        # Perform Eigen Summation to update value function\n",
    "        sum = np.einsum('ijk,ijk -> ij', Transitions, Rewards + .8*v_function)\n",
    "\n",
    "        # Increase Dimensionality of the policy to be (s_count, a_count) from (s_count,)\n",
    "        reshaped_policy = np.zeros((s_count, a_count))\n",
    "        reshaped_policy[np.arange(s_count), policy] = 1\n",
    "\n",
    "        # Calculate new value function for the current policy\n",
    "        v_function = np.sum(reshaped_policy * sum, 1)\n",
    "        \n",
    "        # If the difference between the previous and current is less than delta, the policy goes through\n",
    "        if np.max(np.abs(v_function - previous_v_function)) < 0.001:\n",
    "            break\n",
    "    \n",
    "    # Calculate the new Policy\n",
    "    sum = np.einsum('ijk,ijk -> ij', Transitions, Rewards + .8*v_function)\n",
    "    \n",
    "    # I found that using argmax here allowed for the best policy to be outputted\n",
    "    policy = np.argmax(sum, 1)\n",
    "    \n",
    "    # Evaluate the Policy by comparing it to the previous policy  \n",
    "    if np.array_equal(policy, previous_policy):\n",
    "        break\n",
    "\n",
    "# Display Results\n",
    "\n",
    "print(\"Optimal Policy: \")\n",
    "\n",
    "# Convert numeric values into action values: N, S, E, W, P, and D\n",
    "print(np.array([mapping[action] for action in policy]))\n",
    "print(\"Number of Policy Iterations: \" + str(i))\n",
    "\n",
    "print(\"Optimal Value Function: \")\n",
    "print(v_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
